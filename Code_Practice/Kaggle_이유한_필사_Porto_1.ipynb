{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNjJ+cZQnhIEgAvVopB9J7W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d12OQ5C52AaV"},"outputs":[],"source":["# Loading packages\n","\n","import pandas as pd\n","import numpy as np\n","import maplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import Imputer\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.utils import shuffle\n","from sklearn.ensemble import RandomForestClassifier\n","\n","pd.set_potion('display.max_columns' , 100)"]},{"cell_type":"code","source":["# Loading data\n","\n","train = pd.read_csv('../input/train.csv')\n","test = pd.read_csv('../input/test.csv')"],"metadata":{"id":"UJ2xCBae2_LT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data at first sight\n","\n","train.head()\n","train.tail()\n","train.shape\n","train.drop_duplicates()\n","train.shape\n","test.shape"],"metadata":{"id":"zhlHQEml3FLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train.info()"],"metadata":{"id":"TebkI44S3YDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metadata\n","\n","data = []\n","for f in train.columns:\n","\n","    if f == 'target':\n","        role = 'target'\n","    elif f == 'id':\n","        role = 'id'\n","    else:\n","        role = 'input'\n","\n","    if 'bin' in f or f == 'target':\n","        level = 'binary'\n","    elif 'cat' in f or f == 'id':\n","        level = 'nominal'\n","    elif train[f].dtype == float:\n","        level = 'interval'\n","    elif train[f].dtype == int:\n","        level = 'ordinal'\n","\n","    keep = True\n","    if f == 'id':\n","        keep = False\n","\n","    dtype = train[f].dtype\n","\n","    f_dict = {\n","        'varname' : f ,\n","        'role' : role ,\n","        'level' : level ,\n","        'keep' : keep ,\n","        'dtype' : dtype\n","    }\n","    data.append(f_dict)\n","\n","meta = pd.DataFrame(\n","    data ,\n","    columns = ['varname' , 'role' , 'level' , 'keep' , 'dtype']\n",")\n","\n","meta.set_index('varname' , inplace = True)"],"metadata":{"id":"rVYtoqtr3aJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meta"],"metadata":{"id":"nRGW8_5O4IRF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meta[(meta.level == 'nominal') & (meta.keep)].index"],"metadata":{"id":"DhPR98XN4LTB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(\n","    {\n","        'count' : meta.groupby(['role' , 'level'])['role'].size()\n","    }\n",").reset_index()"],"metadata":{"id":"6vzD9ppA4XVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Descriptive statistics\n","\n","# Interval variables\n","v = meta[(meta.level == 'interval') & (meta.keep)].index\n","train[v].describe()"],"metadata":{"id":"BdrExWUt4hdE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ordinal variables\n","\n","v = meta[(meta.level == 'ordinal') & (meta.keep)].index\n","train[v].describe()"],"metadata":{"id":"In1SU-Wg5RDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Binary variables\n","\n","v = meta[(meta.level == 'binary') & (meta.keep)].index\n","train[v].describe()"],"metadata":{"id":"Gvx_Fu2q5nCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Handling imbalanced classes\n","\n","desired_apriori = 0.10\n","\n","idx_0 = train[train.target == 0].index\n","idx_1 = train[train.target == 1].index\n","\n","nb_0 = len(train.loc[idx_0])\n","nb_1 = len(train.loc[idx_1])\n","\n","undersampling_rate = ((1 - desired_apriori) * nb_1) / (nb_0 * desired_apriori)\n","undersampled_nb_0 = int(undersampling_rate * nb_0)\n","\n","print('Rate to undersample records with target = 0 : {}'.format(undersampling_rate))\n","print('Number of records with target = 0 after undersampling : {}'.format(undersampled_nb_0))\n","\n","undersampled_idx = shuffle(idx_0 , random_state = 37 , n_samples = undersampled_nb_0)\n","\n","idx_list = list(undersampled_idx) + list(idx_1)\n","\n","train = train.loc[idx_list].reset_index(drop = True)"],"metadata":{"id":"IOdU-E0g6Rj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Quality Checks\n","\n","# Checking missing values\n","\n","vars_with_missing = []\n","\n","for f in train.columns:\n","    missings = train[train[f] == -1][f].count()\n","    if missings > 0:\n","        vars_with_missing.append(f)\n","        missings_perc = missings/train.shape[0]\n","\n","        print('Variable {} has records ({:.2%}) with missing values'.format(f , missings , missings_perc))\n","\n","print('In total , there are {} variables with missing values'.format(len(vars_with_missing)))"],"metadata":{"id":"6VWXO16gd632"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dropping the variables with too many missing values\n","\n","vars_to_drop = ['ps_car_03_cat' , 'ps_car_05_cat']\n","train.drop(vars_to_drop , inplace = True , axis = 1)\n","meta.loc[(vars_to_drop) , 'keep'] = False\n","\n","mean_imp = Imputer(missing_values = -1 , strategy = 'mean' , axis = 0)\n","mode_imp = Imputer(missing_values = -1 , strategy = 'most_frequent' , axis = 0)\n","\n","train['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\n","train['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\n","train['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\n","train['ps_car_11'] = mean_imp.fit_transform(train[['ps_car_11']]).ravel()"],"metadata":{"id":"J9gs1nCteWqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the cardinality of the categorical variables\n","\n","v = meta[(meta.level == 'nominal') & (meta.keep)].index\n","\n","for f in v:\n","    dist_values = train[f].value_counts().shape[0]\n","    print('Variable {} has {} distinct values'.format(f , dist_values))"],"metadata":{"id":"9VA3DYPyfELw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_noise(series , noise_level):\n","    return series * (1 + noise_level * np.random.randn(len(series)))\n","\n","def target_encoder(\n","    trn_series = None ,\n","    tst_series = None ,\n","    min_samples_leaf = 1 ,\n","    smoothing = 1 ,\n","    noise_level = 0\n","):\n","    assert len(trn_series) == len(target)\n","    assert trn_series.name == tst_series.name\n","    temp = pd.concat([trn_series , target] , axis = 1)\n","\n","    averages = temp.gropuby(by = trn_series.name)[target.name].agg(['mean' , 'count'])\n","\n","    smoothing = 1 / (1 + np.exp(-(averages['count'] - min_samples_leaf) / smoothing))\n","\n","    prior = target.mean()\n","\n","    averages[target.name] = prior * (1 - smoothing) + averages['mean'] * smoothing\n","    averages.drop(['mean' , 'count'] , axis = 1 , inplace = True)\n","\n","    ft_trn_series = pd.merge(\n","        trn_series.to_frame(trn_series.name) ,\n","        averages.reset_index().rename(columns = {'index' : target.name ,\n","                                                 target.name : 'average'}) ,\n","        on = trn_series.name ,\n","        how = 'left'\n","    )['average'].rename(trn_series.name + '_mean').fillna(prior)\n","\n","    ft_trn_series.index = trn_series.index\n","    ft_tst_series = pd.merge(\n","        tst_series.to_frame(tst_series.name) ,\n","        averages.reset_index().rename(columns = {'index' : target.name ,\n","                                                 target.name = 'average'}) ,\n","        on = tst_series.name ,\n","        how = 'left'\n","    )['average'].rename(trn_series.name + '_mean').fillna(prior)\n","\n","    ft_tst_series.index = tst_series.index\n","    return add_noise(ft_trn_series , noise_level) ,\n","    add_noise(ft_tst_series , noise_level)"],"metadata":{"id":"Hcghky_8fb0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_encoded , test_encoded = target_encode(\n","    train['ps_car_11_cat'] ,\n","    test['ps_car_11_cat'] ,\n","    target = train.target ,\n","    min_samples_leaf = 100 ,\n","    smoothing = 10 ,\n","    noise_level = 0.01\n",")\n","\n","train['ps_car_11_cat_te'] = train_encoded\n","train.drop('ps_car_11_cat' , axis = 1 , inplace = True)\n","meta.loc['ps_car_11_cat' , 'keep'] = False\n","test['ps_car_11_cat_te'] = test_encoded\n","test.drop('ps_car_11_cat' , axis = 1 , inplace = True)"],"metadata":{"id":"8XKcrlXskeMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EDA\n","\n","# Categorical variables\n","\n","v = meta[(meta.level == 'nominal') & (meta.keep)].index\n","\n","for f in v:\n","    plt.figure()\n","    fig , ax = plt.subplots(figsize = (20 , 10))\n","\n","    cat_perc = train[[f , 'target']].groupby([f] , as_index = False).mean()\n","    cat_perc.sort_values(by = 'target'  , ascending = False , inplace = True)\n","\n","    sns.barplot(ax = ax , x = f , y = 'target' , data = cat_perc , order = cat_perc[f])\n","    plt.ylabel('% target' , fontsize = 18)\n","    plt.xlabel(f , fontsize = 18)\n","    plt.tick_params(axis = 'both' , which = 'major' , labelsize = 18)\n","    plt.show()"],"metadata":{"id":"XWboxjV7k5bS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Interval variables\n","\n","def corr_heatmap(v):\n","    correlations = train[v].corr()\n","\n","    cmap = sns.diverging_palette(220 , 10 , as_cmap = True)\n","\n","    fig , ax = plt.subplots(figsize = (10 , 10))\n","    sns.heatmap(correlations , cmap = cmap , vmax = 1.0 , center = 0 , fmt = '.2f' ,\n","                square = True , linewidths = .5 , annot = True ,\n","                cbar_kws = {'shrink' : .75})\n","    plt.show()\n","\n","v = meta[(meta.level == 'interval') & (meta.keep)].index\n","corr_heatmap(v)"],"metadata":{"id":"v366M8aU-jfM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["s = train.sample(frac = 0.1)"],"metadata":{"id":"54k888GM_WRv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.lmplot(x = 'ps_reg_02' , y = 'ps_reg_03' , data = s ,\n","           hue = 'target' , palette = 'Set1' , scatter_kws = {'alpha' : 0.3})\n","plt.show()"],"metadata":{"id":"I2NjT9Wq_mVS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.lmplot(x = 'ps_car_12' , y = 'ps_car_13' , data = s , hue = 'target' ,\n","           palette = 'Set1' , scatter_kws = {'alpha' : 0.3})\n","plt.show()"],"metadata":{"id":"jwf3D48j_wEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.lmplot(x = 'ps_car_12' , y = 'ps_car_14' , data = s , hue = 'target' ,\n","           palette = 'Set1' , scatter_kws = {'alpha' : 0.3})\n","plt.show()"],"metadata":{"id":"eafcj4ui_5Jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sns.lmplot(x = 'ps_car_15' , y = 'ps_car_13' , data = s , hue = 'target' ,\n","           palette = 'Set1' , scatter_kws = {'alpha' : 0.3})\n","plt.show()"],"metadata":{"id":"qLbdyWDdAAlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Checking the correlations between ordinal variables\n","\n","v = meta[(meta.level == 'ordinal') & (meta.keep)].index\n","corr_heatmap(v)"],"metadata":{"id":"bP7lmJs7AKPR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature engineering\n","\n","# Creating dummy variables\n","\n","v = meta[(meta.level == 'nominal') & (meta.keep)].index\n","print('Before dummification we have {} variables in train'.format(train.shape[1]))\n","train = pd.get_dummies(train , columns = v , drop_first = True)\n","print('After dummification we have {} variables in train'.format(train.shape[1]))"],"metadata":{"id":"ZujYu2fLAU4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating interaction variables\n","\n","v = meta[(meta.level == 'interval') & (meta.keep)].index\n","poly = PolynomialFeatures(degree = 2 , interaction_only = False , include_bias = False)\n","interactions = pd.DataFrame(data = poly.fit_transform(train[v]) , columns = poly.get_feature_names(v))\n","interactions.drop(v , axis = 1 , inplace = True)\n","\n","print('Before creating interactions we have {} variables in train'.format(train.shape[1]))\n","train = pd.concat([train , interactions] , axis = 1)\n","print('After creating interactions we have {} variables in train'.format(train.shape[1]))"],"metadata":{"id":"WHKjpfgRAx-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature selection\n","\n","# Removing features with low or zero variance\n","\n","selector = VarianceThreshold(threshold = .01)\n","selector.fit(train.drop(['id' , 'target'] , axis = 1))\n","\n","f = np.vectorize(lambda x : not x)\n","\n","v = train.drop(['id' , 'target'] , axis = 1).columns[f(selector.get_support())]\n","print('{} variables have too low variance.'.format(len(v)))\n","print('These varialbes are {}'.format(list(v)))"],"metadata":{"id":"m8xFT8TeBia6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Selecting features with a Random Forest and SelectFromModel\n","\n","X_train = train.drop(['id' , 'target'] , axis = 1)\n","y_train = train['target']\n","\n","feat_labels = X_train.columns\n","\n","rf = RandomForestClassifier(n_estimators = 1000 , random_state = 0 , n_jobs = -1)\n","\n","rf.fit(X_train , y_train)\n","importances = rf.feature_importaces_\n","\n","indices = np.argsort(rf.feature_importances_)[::-1]\n","\n","for f in range(X_train.shape[1]):\n","    print(\"%2d) %-*s %f\" % (f + 1 , 30 , feat_labels[indices[f]] , importances[indices[f]]))"],"metadata":{"id":"CNOd-jUbDJy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sfm = SelectFromModel(rf , threshold = 'median' , prefit = True)\n","print('Number of features before selection: {}'.format(X_train.shape[1]))\n","n_features = sfm.transform(X_train).shape[1]\n","print('Number of features after selection: {}'.format(n_features))\n","selected_vars = list(feat_labels[sfm.get_support()])"],"metadata":{"id":"nSoUEQGEDzn9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = train[selected_vars + ['target']]"],"metadata":{"id":"oimQm_j_EJg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature scaling\n","\n","scaler = StandardScaler()\n","scaler.fit_transform(train.drop(['target'] , axis = 1))"],"metadata":{"id":"G3Y2pQEBEL_3"},"execution_count":null,"outputs":[]}]}